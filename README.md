# DQN-and-its-variants

---

# HW 4.1
## Naive DQN 在 Static 與 Random 模式下的表現分析報告

### 實驗目標

* 比較 Naive DQN 在 Static 模式與 Random 模式下的學習表現差異。
* 檢驗加入 **Experience Replay Buffer** 與 **Target Network** 等改良機制對 DQN 訓練穩定性與效果的影響。
* 分析兩種模式下性能差異的原因，探討改良後性能提升的機制與原因。

### 實作內容說明

Naive DQN 基於深度 Q 學習，在每個時間步使用神經網絡估計各動作的 Q 值，並以 ε-greedy 策略選擇動作，之後根據經典 Q 學習 (Bellman Equation) 更新網絡參數。下列為對 Naive DQN 的改良版本描述：

* **Naive DQN（基礎版）**：直接將觀測到的狀態作為網絡輸入，輸出對應動作的 Q 值，並在每一步使用最新的網絡輸出計算訓練目標。此版本無額外機制，容易受到連續樣本相關性與目標值變動的影響。
* **加入經驗回放 (Experience Replay Buffer)**：將 agent 產生的 (狀態, 動作, 獎勵, 下個狀態) 序列存入緩衝區，訓練時隨機抽取批次 (minibatch) 進行更新。經驗回放使得訓練樣本「隨機化」，消除了連續樣本之間的相關性，從而平滑數據分佈並穩定訓練過程。正如文獻所述，隨機抽樣有助於去相關化，顯著提升 DQN 訓練的穩定度和收斂性。
* **加入目標網絡 (Target Network)**：建立一個與主網絡結構相同但參數延遲更新的副網絡，僅在每隔固定步數才將主網絡的權重複製過來。目標網絡用於計算 Q-learning 的目標值，這樣訓練時目標不會隨主網絡權重立即變化，避免強烈的自我反饋循環。目標網絡可產生更穩定的目標 Q 值，降低訓練中的震盪，從而提高收斂性。

### 實驗結果比較

Naive DQN 在 **Static 模式**和 **Player 模式**下的學習曲線與最終表現如圖所示。在 Static 模式中，初始狀態固定，原始 Naive DQN 可穩定地學到一定策略，隨著訓練逐漸收斂，累積獎勵逐步提升，表現尚可接受。

Static mode

![image](https://github.com/user-attachments/assets/8ab52a2b-edfd-45ae-b9dc-fd84812e106d) 

Player mode

![image](https://github.com/user-attachments/assets/3c177642-c9cb-4122-a006-ad1ae780837c) 


相較之下，在 **Random 模式**（每次 episode 初始狀態隨機）中，原始 Naive DQN 的訓練表現明顯較差。由於環境條件每次變化，訓練時的狀態分佈更加多樣，原始 DQN 容易陷入震盪，學習曲線波動大且最終平均獎勵低下，無法穩定找到最佳策略。
![image](https://github.com/user-attachments/assets/50be9bb5-744e-4472-b19c-de1eb24b6201)


將 **經驗回放** 機制加入 Naive DQN 後，在 Random 模式下的學習效果有所改善。由於訓練時使用了多樣化的歷史樣本，樣本間相關性降低，使訓練曲線更加平滑，最終策略也更佳。經驗回放的應用令累積獎勵比原始版本提高，學習過程更加穩定。
![image](https://github.com/user-attachments/assets/e16a4ce7-3666-4255-ba0d-2abaa7cd6b31)


進一步加入 **目標網絡** 後，Random 模式下 DQN 的表現進一步提升。目標網絡的週期性更新使得訓練目標更穩定，訓練過程中 Q 值估計變化較小，收斂過程更加平緩，最終性能顯著優於僅用經驗回放的情況，並接近 Static 模式的水平。
![image](https://github.com/user-attachments/assets/cde8e4e9-a858-4132-a344-734090623cf0)

最後加入防止撞墻機制後最終結果收斂得非常好。
![image](https://github.com/user-attachments/assets/1ce2b0a4-6416-4930-893e-6cb37d6bc97c)


### 結果分析與推測原因

Naive DQN 在 Random 模式下表現不佳，主要原因可歸結為環境變化加劇了學習的困難度。強化學習演算法通常假設環境是平穩的，若環境每次初始條件不同，相當於增加了非平穩性，這使得無改良的 DQN 訓練過程震盪加劇且難以收斂。具體來說，不使用經驗回放時，訓練過程中樣本間的相關性很強，神經網絡容易過度擬合到連續走過的狀態序列，容易陷入局部最優，從而導致學習不穩定。此外，若未使用目標網絡，每一步更新同時影響當前和下一步的 Q 值估計，訓練目標會隨網絡參數即時改變，形成劇烈的反饋循環，使得 Q 值估計震盪，收斂困難。

透過加入經驗回放和目標網絡，上述問題得到有效緩解。經驗回放機制使得訓練時從歷史緩衝區隨機取樣，增強了訓練樣本的多樣性，降低了批次更新之間的相關性，這對於穩定訓練非常關鍵。目標網絡則將 Q 值目標的計算從當前網絡參數中隔離出來，使目標值在多次更新間保持相對固定，大幅度減少了估計中的抖動。這兩種改良共同提升了 DQN 在隨機環境下的學習穩定度和最終表現。

### 小結

本報告實驗結果顯示，Naive DQN 在靜態（Static）環境下可以學習到一定程度的策略，但面對隨機（Random）環境時表現不佳。引入經驗回放緩衝區和目標網絡後，DQN 的訓練穩定性顯著提高，在隨機環境中的性能也大幅提升。經驗回放提供了更多樣化的訓練樣本並去除了連續樣本間的相關性，目標網絡則通過固定目標值大幅降低了估計的震盪。這些技術是深度強化學習成功應用的關鍵因素。實驗過程中，我們深刻體會到穩定學習機制對 DQN 收斂的重要性。未來可以嘗試引入其他改良技術（如 Double DQN、Dueling DQN 等）來進一步提升模型在更複雜隨機環境下的表現和收斂速度。
